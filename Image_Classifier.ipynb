{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5092089,"sourceType":"datasetVersion","datasetId":2956926},{"sourceId":6634841,"sourceType":"datasetVersion","datasetId":3830245},{"sourceId":8318945,"sourceType":"datasetVersion","datasetId":4941183},{"sourceId":8320215,"sourceType":"datasetVersion","datasetId":4942115}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:44:15.483987Z","iopub.execute_input":"2024-05-05T08:44:15.484508Z","iopub.status.idle":"2024-05-05T08:44:16.346373Z","shell.execute_reply.started":"2024-05-05T08:44:15.484471Z","shell.execute_reply":"2024-05-05T08:44:16.345115Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# write your dataset directory path\ndata_dir = '/kaggle/input/real-life-violence/Real life violence dataset'\nbatch_size = 32\nimage_width = 224\nimage_height = 224\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:44:19.373103Z","iopub.execute_input":"2024-05-05T08:44:19.374706Z","iopub.status.idle":"2024-05-05T08:44:19.381964Z","shell.execute_reply.started":"2024-05-05T08:44:19.374635Z","shell.execute_reply":"2024-05-05T08:44:19.380193Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Create ImageDataGenerator for training data with validation split\ntrain_data_augmentor = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.1,\n    zoom_range=0.05,\n    horizontal_flip=True,\n    validation_split=0.2\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:44:22.448386Z","iopub.execute_input":"2024-05-05T08:44:22.448864Z","iopub.status.idle":"2024-05-05T08:44:22.455311Z","shell.execute_reply.started":"2024-05-05T08:44:22.448831Z","shell.execute_reply":"2024-05-05T08:44:22.453675Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load training data from directory\ntrain_data = train_data_augmentor.flow_from_directory(\n    data_dir,\n    target_size=(image_width, image_height),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training'\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:44:25.307707Z","iopub.execute_input":"2024-05-05T08:44:25.308143Z","iopub.status.idle":"2024-05-05T08:44:31.832423Z","shell.execute_reply.started":"2024-05-05T08:44:25.308111Z","shell.execute_reply":"2024-05-05T08:44:31.830478Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found 11245 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load validation data from directory\nvalidation_data = train_data_augmentor.flow_from_directory(\n    data_dir,\n    target_size=(image_width, image_height),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='validation'\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:44:33.988142Z","iopub.execute_input":"2024-05-05T08:44:33.989516Z","iopub.status.idle":"2024-05-05T08:44:37.088598Z","shell.execute_reply.started":"2024-05-05T08:44:33.989458Z","shell.execute_reply":"2024-05-05T08:44:37.087685Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Found 2811 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check class indices\nprint(train_data.class_indices)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:44:38.139753Z","iopub.execute_input":"2024-05-05T08:44:38.140963Z","iopub.status.idle":"2024-05-05T08:44:38.147930Z","shell.execute_reply.started":"2024-05-05T08:44:38.140917Z","shell.execute_reply":"2024-05-05T08:44:38.146479Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'nonviolence': 0, 'violence': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import MobileNetV2\n\n# Path to the MobileNetV2 weights file\nweights_path = \"/kaggle/input/mobilenetv2-h5/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\"\n\n# Load MobileNetV2 pre-trained on ImageNet without including the top classification layer\nbase_model = MobileNetV2(weights=weights_path, include_top=False, input_shape=(image_width, image_height, 3))\n\n# Freeze the base model's layers so they are not trainable\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Add additional layers on top of the base model\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(1, activation='sigmoid')(x)\n\n# Create the final model\nmodel = Model(inputs=base_model.input, outputs=output)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:45:40.851195Z","iopub.execute_input":"2024-05-05T08:45:40.852537Z","iopub.status.idle":"2024-05-05T08:45:42.101319Z","shell.execute_reply.started":"2024-05-05T08:45:40.852473Z","shell.execute_reply":"2024-05-05T08:45:42.099863Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:45:50.709878Z","iopub.execute_input":"2024-05-05T08:45:50.711655Z","iopub.status.idle":"2024-05-05T08:45:50.733636Z","shell.execute_reply.started":"2024-05-05T08:45:50.711608Z","shell.execute_reply":"2024-05-05T08:45:50.731525Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define number of epochs\nepochs = 8\n\n# Train the model with augmented data\nhistory = model.fit(\n    train_data,\n    verbose=1,\n    epochs=epochs,\n    validation_data=validation_data,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:45:53.789501Z","iopub.execute_input":"2024-05-05T08:45:53.790660Z","iopub.status.idle":"2024-05-05T09:37:24.419634Z","shell.execute_reply.started":"2024-05-05T08:45:53.790610Z","shell.execute_reply":"2024-05-05T09:37:24.418368Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1/8\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 1s/step - accuracy: 0.8315 - loss: 0.4104 - val_accuracy: 0.8844 - val_loss: 0.3275\nEpoch 2/8\n\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - accuracy: 0.9344 - loss: 0.1679 - val_accuracy: 0.8300 - val_loss: 0.4344\nEpoch 3/8\n\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 1s/step - accuracy: 0.9507 - loss: 0.1286 - val_accuracy: 0.8634 - val_loss: 0.3772\nEpoch 4/8\n\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 1s/step - accuracy: 0.9548 - loss: 0.1137 - val_accuracy: 0.8723 - val_loss: 0.3912\nEpoch 5/8\n\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 1s/step - accuracy: 0.9631 - loss: 0.1006 - val_accuracy: 0.8648 - val_loss: 0.3599\nEpoch 6/8\n\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 1s/step - accuracy: 0.9639 - loss: 0.0906 - val_accuracy: 0.8613 - val_loss: 0.4068\nEpoch 7/8\n\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 1s/step - accuracy: 0.9697 - loss: 0.0714 - val_accuracy: 0.8321 - val_loss: 0.5131\nEpoch 8/8\n\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 1s/step - accuracy: 0.9740 - loss: 0.0630 - val_accuracy: 0.8801 - val_loss: 0.3884\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import save_model\nfrom IPython.display import FileLink\n\n# Save the model with .h5 extension\nmodel_path = \"./bablu_model.h5\"\nmodel.save(model_path)\n\n# Create a download link for the model\nFileLink(model_path)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T09:43:50.111710Z","iopub.execute_input":"2024-05-05T09:43:50.112150Z","iopub.status.idle":"2024-05-05T09:43:50.405984Z","shell.execute_reply.started":"2024-05-05T09:43:50.112115Z","shell.execute_reply":"2024-05-05T09:43:50.404350Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/bablu_model.h5","text/html":"<a href='./bablu_model.h5' target='_blank'>./bablu_model.h5</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}